# -------------------------------------------------------------------------------------------------------------------
# 1 Master Controller VM 4 CPU's 8GB RAM 40GB HDD thin provisioned - K8S not installed.
# 5 VM's 2CPU's 2GB RAM 20GB HDD thin provisioned - K8S not installed.
# CentOS 8 Stream Install URL:  URI: http://mirror.centos.org/centos/8-stream/BaseOS/x86_64/os/
# Select Minimal install - assign .160 address and complete centos install.  Convert VM to Template.
# -------------------------------------------------------------------------------------------------------------------
# -------------------------------------------------------------------------------------------------------------------
# # Edit line 18 for each Node - Copy 18-232, paste into Master Controller - Copy 18-158, paste into other M/W Nodes
# -------------------------------------------------------------------------------------------------------------------

# Check MAC Address for duplicates
# ip link

# To check the product_uuid and compare
# cat /sys/class/dmi/id/product_uuid

# Edit HOSTS - Change Hostname(s) on following line(s)
hostnamectl set-hostname pac-k8s-master0

cat <<EOF>> /etc/hosts
192.168.178.161 pac-k8s-master0
192.168.178.162 pac-k8s-master1
192.168.178.163 pac-k8s-master2
192.168.178.164 pac-k8s-worker0
192.168.178.165 pac-k8s-worker1
192.168.178.166 pac-k8s-worker2
192.168.178.167 pac-k8s-worker3-varna
192.168.178.168 pac-k8s-worker4-varna
EOF

# --------------------------------------------------------------------------------------------------------------------
#  NODE Deployment
# --------------------------------------------------------------------------------------------------------------------

# Update CentOS
dnf -y upgrade
sysctl --system
systemctl daemon-reload

# Disable SELinux enforcement
setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux

# Open Firewall Ports
firewall-cmd --zone=public --permanent --add-port={6443,5701-5711,2379,2380,10250,10251,10252}/tcp
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.161/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.162/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.163/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.164/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.165/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.166/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.167/24 accept'
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=192.168.178.168/24 accept'

# Allow access to the hostâ€™s localhost from the docker container
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=172.17.0.0/16 accept'

# Enable IP masquerade at the firewall
firewall-cmd --add-masquerade --permanent

# Enable transparent masquerading and facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster.
modprobe br_netfilter

# Set bridged packets to traverse iptables rules.
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# Load the new rules
firewall-cmd --reload
sysctl --system
systemctl daemon-reload

# Set bridged packets to traverse iptables rules
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# Disable SWAP to enable the kubelet to work properly
sudo sed -i '/swap/d' /etc/fstab
sudo swapoff -a
sudo rm /etc/fstab

# Load the new rules
firewall-cmd --reload
sysctl --system
systemctl daemon-reload

# --------------------------------------------------------------------------------------------------------------------
# Install Docker #
# --------------------------------------------------------------------------------------------------------------------

## Install Containerd.io ##
sudo dnf -y install https://download.docker.com/linux/centos/8/x86_64/stable/Packages/containerd.io-1.4.3-3.1.el8.x86_64.rpm -y
sudo sysctl --system
sudo systemctl daemon-reload
systemctl start containerd
systemctl enable containerd

## Install Docker 19.03 ##
sudo dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
# Install specific version of docker (19.03 required by K8S)
sudo dnf -y install docker-ce-3:19.03.14-3.el8
# sudo dnf -y install https://download.docker.com/linux/centos/8/x86_64/stable/Packages/docker-ce-19.03.14-3.el8.x86_64.rpm
# sudo dnf -y install https://download.docker.com/linux/centos/8/x86_64/stable/Packages/docker-ce-cli-19.03.14-3.el8.x86_64.rpm
sudo dnf provides tc -y
sudo dnf install iproute-tc -y

# Add Docker Repository - Install most recent version
# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
# dnf install docker-ce --nobest -y

# Perm change docker cgroupdriver to systemd
cat <<EOF>> /usr/lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
EOF

cat <<EOF>> /etc/systemd/system/docker.service.d/execstart_override.conf
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
EOF

# Start and enable Docker at startup
sudo sysctl --system
sudo systemctl daemon-reload
sudo systemctl start docker
sudo systemctl enable docker

# --------------------------------------------------------------------------------------------------------------------
# Install Kubernetes #
# --------------------------------------------------------------------------------------------------------------------

# Add Kubernetes Repository
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Install Kubernetes
dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# Start and enable kubernetes
sudo sysctl --system
sudo systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet

# Pull images
kubeadm config images pull

# --------------------------------------------------------------------------------------------------------------------
# ###########################  Copy to here for Worker Node Deployment ############################################# #
# --------------------------------------------------------------------------------------------------------------------
#
# --------------------------------------------------------------------------------------------------------------------
# ####################################   Launch Control Plane #   ################################################## #
# --------------------------------------------------------------------------------------------------------------------

# Single Control Plane
# sudo kubeadm init --pod-network-cidr 192.168.0.0/16

# High Availability Control Plane
sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --control-plane-endpoint "192.168.178.161:6443" --upload-certs

# Install Calico Container Network Interface
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# Deploy Kubernetes Dashboard
sudo kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
sudo sysctl --system
sudo systemctl daemon-reload

# Confirm that it is working by checking that the CoreDNS pod is running by typing:
# sudo kubectl get pods --all-namespaces

# Create Service Account
kubectl create serviceaccount dashboard-pete-sa

# Bind Service Account
kubectl create clusterrolebinding dashboard-pete-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-pete-sa

# List secrets
kubectl get secrets

# kubectl describe secret - access token:

# Start Dashboard Host Server

# kubectl proxy

# To access the Dashboard, open a web browser on the node where the proxy is running and navigate to
# http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login.
# and paste the above secret to log in

# --------------------------------------------------------------------------------------------------------------------

# Before joining cluster members - Taint Master node, to act as worker node.  After installing Grafana and Jenkins - remove Taint. 
# kubectl get nodes
kubectl taint nodes pac-k8s-master0 node-role.kubernetes.io/master-
# Remove Taint: 
# kubectl taint node --all node-role.kubernetes.io/master:NoSchedule

# Install Helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm repo add stable https://charts.helm.sh/stable

# --------------------------------------------------------------------------------------------------------------------
# Install Prometheus and Grafana #
# --------------------------------------------------------------------------------------------------------------------
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install --generate-name prometheus-community/kube-prometheus-stack

# helm upgrade --install --wait --timeout 20 prometheus-stack prometheus-community/kube-prometheus-stack
# Wait Patiently

# Install SQLite
sudo dnf install -y sqlite

# --------------------------------------------------------------------------------------------------------------------
# Join other Master/Worker Nodes to Cluster
# --------------------------------------------------------------------------------------------------------------------
# Look up grafana service port on K8S Dashboard
# Reset Grafana default password - set new password
# --------------------------------------------------------------------------------------------------------------------
# cd /
# find -name "grafana.db"
# cd to location of grafada.db
# sudo sqlite3 grafana.db
# update user set password = '59acf18b94d7eb0694c61e60ce44c110c7a683ac6a8f09580d626f90f4a242000746579358d77dd9e570e83fa24faa88a8a6', salt = 'F3FAxVm33R' where login = 'admin';
# sqlite> .exit

# Change service type for Prometheus and Grafana services to NodePort, and wait for port number to self assign.
# Look up service port numbers and go to master1 server IP:service port
# --------------------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------------------
# Clean Up
curl -Ls http://bit.ly/clean-centos-disk-space | sudo bash
# Trim Log files
find /var -name "*.log" \( \( -size +50M -mtime +7 \) -o -mtime +30 \) -exec truncate {} --size 0 \;
yum clean all
rm -rf /var/cache/yum
rm -rf /var/tmp/yum-*
package-cleanup --quiet --leaves -y
package-cleanup --quiet --leaves | xargs yum remove -y
rm -rf /root/.wp-cli/cache/*
rm -rf /home/*/.wp-cli/cache/*
(( $(rpm -E %{rhel}) >= 8 )) && dnf remove $(dnf repoquery --installonly --latest-limit=-2 -q) -y
(( $(rpm -E %{rhel}) <= 7 )) && package-cleanup --oldkernels --count=2 -y
(( $(rpm -E %{rhel}) >= 8 )) && dnf remove $(dnf repoquery --installonly --latest-limit=-1 -q) --nobest --skip-broken -y
(( $(rpm -E %{rhel}) <= 7 )) && package-cleanup --oldkernels --count=1 -y
rm -rf /root/.composer/cache
rm -rf /home/*/.composer/cache
find -regex ".*/core\.[0-9]+$" -delete
find /home/*/public_html/ -name error_log -delete
rm -rf /root/.npm /home/*/.npm /root/.node-gyp /home/*/.node-gyp /tmp/npm-*
rm -rf /var/cache/mock/* /var/lib/mock/*
